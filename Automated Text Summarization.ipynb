{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec751a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to tokenize the text and and identify the occurence of important words then we will calculate the importance of each word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f057739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we will assign weights to each sentences based on the importance of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df427be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally we will rank the sentences and select top rank sentences from the text body to form the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e566b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Once upon a time, a farmer had a goose that laid a golden egg every day. The farmer used to sell that egg and earn enough money to meet their family's day-to-day needs. One day, the farmer thought that if he could get more such golden eggs and make a lot of money and become a wealthy person. The farmer decided to cut the goose and remove all the golden eggs from its stomach. As soon as they killed the bird and opened the gooseâ€™s stomach, they found no eggs. The foolish farmer realized they had destroyed their last resource out of greed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ada6413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding how many words are in text\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df7db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries \n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcf71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.21.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ragha\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefd26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0733426",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5d4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting text from the token and convert them to lower.so we are printing the lower case version of the token if it is not a \n",
    "# stop word as well as if it is not a punctuation or a new line and store the tokens in a variable called tokens\n",
    "tokens = [token.text.lower() for token in doc\n",
    "         if not token.is_stop and\n",
    "         not token.is_punct and\n",
    "         token.text !='\\n']\n",
    "#this code performs tokensisation and removes stoppers from the text \n",
    "#this removes punctuation marks such as 'or,is etc' and new line charaters from the text leaving us a list of meaningful words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e964dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'farmer',\n",
       " 'goose',\n",
       " 'laid',\n",
       " 'golden',\n",
       " 'egg',\n",
       " 'day',\n",
       " 'farmer',\n",
       " 'sell',\n",
       " 'egg',\n",
       " 'earn',\n",
       " 'money',\n",
       " 'meet',\n",
       " 'family',\n",
       " 'day',\n",
       " 'day',\n",
       " 'needs',\n",
       " 'day',\n",
       " 'farmer',\n",
       " 'thought',\n",
       " 'golden',\n",
       " 'eggs',\n",
       " 'lot',\n",
       " 'money',\n",
       " 'wealthy',\n",
       " 'person',\n",
       " 'farmer',\n",
       " 'decided',\n",
       " 'cut',\n",
       " 'goose',\n",
       " 'remove',\n",
       " 'golden',\n",
       " 'eggs',\n",
       " 'stomach',\n",
       " 'soon',\n",
       " 'killed',\n",
       " 'bird',\n",
       " 'opened',\n",
       " 'goose',\n",
       " 'stomach',\n",
       " 'found',\n",
       " 'eggs',\n",
       " 'foolish',\n",
       " 'farmer',\n",
       " 'realized',\n",
       " 'destroyed',\n",
       " 'resource',\n",
       " 'greed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30390b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have printed a list of meaningful words without punctuations , stopper or new line characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6198942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternating approch to removing punctuations or new line characters\n",
    "tokens1=[]#initializing an empty list to store the filterised tokens \n",
    "stopwords=list(STOP_WORDS)#creating a list of stop words using the predefined set of stop words from spacy.refer cell[5]\n",
    "allowed_pos = ['ADJ','PROPN','VERB','NOUN']\n",
    "for token in doc:#iterating each token in doc to check\n",
    "    if token.text in stopwords or token.text in punctuation:#if the token is in the list of stopwords or is a punctuation we skip it\n",
    "        continue\n",
    "    if token.pos_ in allowed_pos:\n",
    "        tokens1.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c05ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'farmer',\n",
       " 'goose',\n",
       " 'laid',\n",
       " 'golden',\n",
       " 'egg',\n",
       " 'day',\n",
       " 'farmer',\n",
       " 'sell',\n",
       " 'egg',\n",
       " 'earn',\n",
       " 'money',\n",
       " 'meet',\n",
       " 'family',\n",
       " 'day',\n",
       " 'day',\n",
       " 'needs',\n",
       " 'day',\n",
       " 'farmer',\n",
       " 'thought',\n",
       " 'golden',\n",
       " 'eggs',\n",
       " 'lot',\n",
       " 'money',\n",
       " 'wealthy',\n",
       " 'person',\n",
       " 'farmer',\n",
       " 'decided',\n",
       " 'cut',\n",
       " 'goose',\n",
       " 'remove',\n",
       " 'golden',\n",
       " 'eggs',\n",
       " 'stomach',\n",
       " 'killed',\n",
       " 'bird',\n",
       " 'opened',\n",
       " 'goose',\n",
       " 'stomach',\n",
       " 'found',\n",
       " 'eggs',\n",
       " 'foolish',\n",
       " 'farmer',\n",
       " 'realized',\n",
       " 'destroyed',\n",
       " 'resource',\n",
       " 'greed']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2138d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#goal is to find most important words in the text by focussing on the words that appear most often so we cna capture the main\n",
    "#summary of the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9694dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#capturing the frequecy of each word from the list of tokens using the counter class from the collection module\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d15b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq=Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d204b42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'time': 1,\n",
       "         'farmer': 5,\n",
       "         'goose': 3,\n",
       "         'laid': 1,\n",
       "         'golden': 3,\n",
       "         'egg': 2,\n",
       "         'day': 4,\n",
       "         'sell': 1,\n",
       "         'earn': 1,\n",
       "         'money': 2,\n",
       "         'meet': 1,\n",
       "         'family': 1,\n",
       "         'needs': 1,\n",
       "         'thought': 1,\n",
       "         'eggs': 3,\n",
       "         'lot': 1,\n",
       "         'wealthy': 1,\n",
       "         'person': 1,\n",
       "         'decided': 1,\n",
       "         'cut': 1,\n",
       "         'remove': 1,\n",
       "         'stomach': 2,\n",
       "         'soon': 1,\n",
       "         'killed': 1,\n",
       "         'bird': 1,\n",
       "         'opened': 1,\n",
       "         'found': 1,\n",
       "         'foolish': 1,\n",
       "         'realized': 1,\n",
       "         'destroyed': 1,\n",
       "         'resource': 1,\n",
       "         'greed': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_freq contains each unique word from the tokens list as a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275221d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding words with the maximum freq\n",
    "max_freq=max(word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe68d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing each words in the word_freq so that each word has a freq between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33b87ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_freq.keys():\n",
    "    word_freq[word]=word_freq[word]/max_freq\n",
    "#this loop iterates through each loop in the word_freq keys and normalize the freq of each words by dividing with max_frew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14b167ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'time': 0.2,\n",
       "         'farmer': 1.0,\n",
       "         'goose': 0.6,\n",
       "         'laid': 0.2,\n",
       "         'golden': 0.6,\n",
       "         'egg': 0.4,\n",
       "         'day': 0.8,\n",
       "         'sell': 0.2,\n",
       "         'earn': 0.2,\n",
       "         'money': 0.4,\n",
       "         'meet': 0.2,\n",
       "         'family': 0.2,\n",
       "         'needs': 0.2,\n",
       "         'thought': 0.2,\n",
       "         'eggs': 0.6,\n",
       "         'lot': 0.2,\n",
       "         'wealthy': 0.2,\n",
       "         'person': 0.2,\n",
       "         'decided': 0.2,\n",
       "         'cut': 0.2,\n",
       "         'remove': 0.2,\n",
       "         'stomach': 0.4,\n",
       "         'soon': 0.2,\n",
       "         'killed': 0.2,\n",
       "         'bird': 0.2,\n",
       "         'opened': 0.2,\n",
       "         'found': 0.2,\n",
       "         'foolish': 0.2,\n",
       "         'realized': 0.2,\n",
       "         'destroyed': 0.2,\n",
       "         'resource': 0.2,\n",
       "         'greed': 0.2})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "173f1de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization using list comprehension\n",
    "sent_token =[sent.text for sent in doc.sents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "020fc23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once upon a time, a farmer had a goose that laid a golden egg every day.',\n",
       " \"The farmer used to sell that egg and earn enough money to meet their family's day-to-day needs.\",\n",
       " 'One day, the farmer thought that if he could get more such golden eggs and make a lot of money and become a wealthy person.',\n",
       " 'The farmer decided to cut the goose and remove all the golden eggs from its stomach.',\n",
       " 'As soon as they killed the bird and opened the gooseâ€™s stomach, they found no eggs.',\n",
       " 'The foolish farmer realized they had destroyed their last resource out of greed.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating sentence imprtance/score based on word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7b2dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once\n",
      "upon\n",
      "a\n",
      "time,\n",
      "a\n",
      "farmer\n",
      "had\n",
      "a\n",
      "goose\n",
      "that\n",
      "laid\n",
      "a\n",
      "golden\n",
      "egg\n",
      "every\n",
      "day.\n",
      "The\n",
      "farmer\n",
      "used\n",
      "to\n",
      "sell\n",
      "that\n",
      "egg\n",
      "and\n",
      "earn\n",
      "enough\n",
      "money\n",
      "to\n",
      "meet\n",
      "their\n",
      "family's\n",
      "day-to-day\n",
      "needs.\n",
      "One\n",
      "day,\n",
      "the\n",
      "farmer\n",
      "thought\n",
      "that\n",
      "if\n",
      "he\n",
      "could\n",
      "get\n",
      "more\n",
      "such\n",
      "golden\n",
      "eggs\n",
      "and\n",
      "make\n",
      "a\n",
      "lot\n",
      "of\n",
      "money\n",
      "and\n",
      "become\n",
      "a\n",
      "wealthy\n",
      "person.\n",
      "The\n",
      "farmer\n",
      "decided\n",
      "to\n",
      "cut\n",
      "the\n",
      "goose\n",
      "and\n",
      "remove\n",
      "all\n",
      "the\n",
      "golden\n",
      "eggs\n",
      "from\n",
      "its\n",
      "stomach.\n",
      "As\n",
      "soon\n",
      "as\n",
      "they\n",
      "killed\n",
      "the\n",
      "bird\n",
      "and\n",
      "opened\n",
      "the\n",
      "gooseâ€™s\n",
      "stomach,\n",
      "they\n",
      "found\n",
      "no\n",
      "eggs.\n",
      "The\n",
      "foolish\n",
      "farmer\n",
      "realized\n",
      "they\n",
      "had\n",
      "destroyed\n",
      "their\n",
      "last\n",
      "resource\n",
      "out\n",
      "of\n",
      "greed.\n"
     ]
    }
   ],
   "source": [
    "sent_score={} #initialize an empty dictionary which will store the score of each sentence calculated based on the frequencies of each word\n",
    "for sent in sent_token:#this loop iterates over each token sent in the sent_token which contains the individual sentences from the text\n",
    "    for word in sent.split(): #this nested loop splits each sentence into individual words and iterates over them\n",
    "        if word.lower() in word_freq.keys():#this condition checks if the lower case version of the word exists as a key in the word_freq dictionary\n",
    "            if sent not in sent_score.keys():#this condition checks if the current sentence is already in the sent score dictionary\n",
    "                sent_score[sent]= word_freq[word]# creates a new entry in the sent_score with the sentence as the key and assigns the frequency of the word\n",
    "            else:\n",
    "                sent_score[sent] +=word_freq[word]#if sentence already present in the sent_score then it increments the score of the current sentence \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529d39fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Once upon a time, a farmer had a goose that laid a golden egg every day.': 2.8,\n",
       " \"The farmer used to sell that egg and earn enough money to meet their family's day-to-day needs.\": 2.4000000000000004,\n",
       " 'One day, the farmer thought that if he could get more such golden eggs and make a lot of money and become a wealthy person.': 3.2,\n",
       " 'The farmer decided to cut the goose and remove all the golden eggs from its stomach.': 3.4000000000000004,\n",
       " 'As soon as they killed the bird and opened the gooseâ€™s stomach, they found no eggs.': 1.0,\n",
       " 'The foolish farmer realized they had destroyed their last resource out of greed.': 1.7999999999999998}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67a59020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb082f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, a farmer had a goose that la...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The farmer used to sell that egg and earn enou...</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, the farmer thought that if he could g...</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The farmer decided to cut the goose and remove...</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As soon as they killed the bird and opened the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The foolish farmer realized they had destroyed...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  Once upon a time, a farmer had a goose that la...    2.8\n",
       "1  The farmer used to sell that egg and earn enou...    2.4\n",
       "2  One day, the farmer thought that if he could g...    3.2\n",
       "3  The farmer decided to cut the goose and remove...    3.4\n",
       "4  As soon as they killed the bird and opened the...    1.0\n",
       "5  The foolish farmer realized they had destroyed...    1.8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(sent_score.items()),columns=['sentence','score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b2d2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the top scoring sentences as per their score\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03839784",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences=3 #selecting top 3 sentences based on thier score\n",
    "n=nlargest(num_sentences,sent_score,key=sent_score.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54670ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The farmer decided to cut the goose and remove all the golden eggs from its stomach. One day, the farmer thought that if he could get more such golden eggs and make a lot of money and become a wealthy person. Once upon a time, a farmer had a goose that laid a golden egg every day.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have performed extractive text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5c37058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import messagebox\n",
    "from tkinter import END\n",
    "\n",
    "# Importing the summarization code\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize_text():\n",
    "    # Get text from the text box\n",
    "    text = text_box.get(\"1.0\", \"end-1c\")\n",
    "    \n",
    "    # Loading spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Tokenization and removing stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if not token.is_stop and not token.is_punct and token.text != '\\n']\n",
    "    # Calculating word frequency\n",
    "    word_freq = Counter(tokens)\n",
    "    if not word_freq:\n",
    "        messagebox.showerror(\"Error\", \"No words found in the text.\")\n",
    "        return\n",
    "    \n",
    "    max_freq = max(word_freq.values())\n",
    "    for word in word_freq.keys():\n",
    "        word_freq[word] = word_freq[word]/max_freq\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    sent_token = [sent.text for sent in doc.sents]\n",
    "\n",
    "    sent_score = {}\n",
    "    for sent in sent_token:\n",
    "        for word in sent.split():\n",
    "            if word.lower() in word_freq.keys():\n",
    "                if sent not in sent_score.keys():\n",
    "                    sent_score[sent] = word_freq[word]\n",
    "                else:\n",
    "                    sent_score[sent] += word_freq[word]\n",
    "\n",
    "    # Select top-scoring sentences based on user input\n",
    "    num_sentences = int(num_sentences_entry.get())\n",
    "    summarized_sentences = nlargest(num_sentences, sent_score, key=sent_score.get)\n",
    "\n",
    "# Display summarized text in the result box\n",
    "    result_box.delete(1.0, END)\n",
    "    result_box.insert(END, \" \".join(summarized_sentences))\n",
    "\n",
    "# GUI setup\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Summarizer\")\n",
    "\n",
    "# Text box for input\n",
    "text_box = scrolledtext.ScrolledText(root, width=50, height=10, wrap=tk.WORD)\n",
    "text_box.pack(pady=10)\n",
    "\n",
    "# Entry field for the number of sentences\n",
    "num_sentences_label = tk.Label(root, text=\"Number of Sentences:\")\n",
    "num_sentences_label.pack()\n",
    "num_sentences_entry = tk.Entry(root, width=10)\n",
    "num_sentences_entry.insert(END, \"3\")  # Default value\n",
    "num_sentences_entry.pack()\n",
    "\n",
    "# Button to summarize\n",
    "summarize_button = tk.Button(root, text=\"Summarize\", command=summarize_text)\n",
    "summarize_button.pack(pady=5)\n",
    "\n",
    "# Result box for output\n",
    "result_box = scrolledtext.ScrolledText(root, width=50, height=5, wrap=tk.WORD)\n",
    "result_box.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcd75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
